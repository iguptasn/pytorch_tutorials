{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a956a60-c782-4a02-95eb-eec5ead07e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46849ca-529d-43a1-94d9-9ec0cee6cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66e0cdf-b1e8-404c-b366-6985b0c6bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets build a graph using pytorch tensors\n",
    "# weight-w and bias-b are parameters that need to be optimized. so we need the gradient of the loss function with respect to them with requires_grad=True\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5,3,requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x,w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f32db056-79b6-4cfb-991b-877789c8d9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4c7429-a97a-4d58-8c64-0c5bbe12aafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x7fd5e59ca0b0>,\n",
       " <BinaryCrossEntropyWithLogitsBackward0 at 0x7fd5e59ca0e0>,\n",
       " None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables with respect to which gradients can be calculated will have the method .grad_fn\n",
    "z.grad_fn, loss.grad_fn, x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18d69066-8908-4083-aea0-22aeddcf0425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[0.3108, 0.0524, 0.1636],\n",
       "         [0.3108, 0.0524, 0.1636],\n",
       "         [0.3108, 0.0524, 0.1636],\n",
       "         [0.3108, 0.0524, 0.1636],\n",
       "         [0.3108, 0.0524, 0.1636]]),\n",
       " tensor([0.3108, 0.0524, 0.1636]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets calculate gradients for loss function with loss.backward(), Note that loss can only be created for scalar outputs. to get the values we use the method w.grad and b.grad\n",
    "loss.backward(), w.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d57424b-32b5-4616-afd4-ef53e4a8fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#once the model has been trained, we can ensure that only forward computations happen and requires_grad is disabled through the following\n",
    "#we can also use this to freeze parameters\n",
    "with torch.no_grad():\n",
    "    z=torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef8ce07a-ef1b-41a7-a60c-f8fe51a4b570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# we can also achieve the same thing with the detach method\n",
    "z = torch.matmul(x,w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ae06c6-0816-4a6a-928a-8ba66848d9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.]], requires_grad=True),\n",
       " tensor([[4., 1., 1., 1.],\n",
       "         [1., 4., 1., 1.],\n",
       "         [1., 1., 4., 1.],\n",
       "         [1., 1., 1., 4.],\n",
       "         [1., 1., 1., 1.]], grad_fn=<TBackward0>),\n",
       " tensor([[4., 2., 2., 2., 2.],\n",
       "         [2., 4., 2., 2., 2.],\n",
       "         [2., 2., 4., 2., 2.],\n",
       "         [2., 2., 2., 4., 2.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case the loss function is also a vector, torch calculates the jacobian product given by vT.J, where v is the same size as the original tensor\n",
    "input = torch.eye(4,5, requires_grad=True)\n",
    "output = (input+1).pow(2).t()\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "input.grad.zero_()\n",
    "output.backward(torch.ones_like(output), retain_graph=True)\n",
    "input, output, input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52a71854-30ee-4812-9398-fb07f2fdbc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([1., 1., 1.]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward(torch.ones_like(z).t())\n",
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0532bb-575d-40b0-8461-4ef467ee71cb",
   "metadata": {},
   "source": [
    "To understand this properly, read https://pytorch.org/docs/stable/notes/autograd.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
